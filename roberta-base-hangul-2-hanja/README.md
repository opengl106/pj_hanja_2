---
library_name: transformers
base_model: klue/roberta-base
tags:
- generated_from_trainer
metrics:
- accuracy
- f1
- precision
- recall
model-index:
- name: roberta-base-hangul-2-hanja
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# roberta-base-hangul-2-hanja

This model is a fine-tuned version of [klue/roberta-base](https://huggingface.co/klue/roberta-base) on the None dataset.
It achieves the following results on the evaluation set:
- Accuracy: 0.9978
- F1: 0.9927
- Loss: 0.0240
- Precision: 0.9940
- Recall: 0.9913

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 32
- eval_batch_size: 32
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 60

### Training results

| Training Loss | Epoch | Step  | Accuracy | F1     | Validation Loss | Precision | Recall |
|:-------------:|:-----:|:-----:|:--------:|:------:|:---------------:|:---------:|:------:|
| 1.6465        | 1.0   | 963   | 0.9618   | 0.8738 | 0.4191          | 0.8922    | 0.8561 |
| 0.3862        | 2.0   | 1926  | 0.9790   | 0.9456 | 0.2318          | 0.9496    | 0.9417 |
| 0.2326        | 3.0   | 2889  | 0.9854   | 0.9723 | 0.1554          | 0.9751    | 0.9696 |
| 0.1607        | 4.0   | 3852  | 0.9901   | 0.9788 | 0.1137          | 0.9797    | 0.9780 |
| 0.1188        | 5.0   | 4815  | 0.9927   | 0.9839 | 0.0863          | 0.9853    | 0.9825 |
| 0.0921        | 6.0   | 5778  | 0.9934   | 0.9832 | 0.0686          | 0.9842    | 0.9822 |
| 0.0726        | 7.0   | 6741  | 0.9941   | 0.9848 | 0.0568          | 0.9857    | 0.9839 |
| 0.06          | 8.0   | 7704  | 0.9956   | 0.9886 | 0.0494          | 0.9909    | 0.9864 |
| 0.0496        | 9.0   | 8667  | 0.9960   | 0.9897 | 0.0421          | 0.9912    | 0.9881 |
| 0.0406        | 10.0  | 9630  | 0.9970   | 0.9916 | 0.0372          | 0.9920    | 0.9913 |
| 0.0351        | 11.0  | 10593 | 0.9965   | 0.9890 | 0.0359          | 0.9916    | 0.9864 |
| 0.0301        | 12.0  | 11556 | 0.9969   | 0.9888 | 0.0331          | 0.9905    | 0.9871 |
| 0.0251        | 13.0  | 12519 | 0.9975   | 0.9914 | 0.0306          | 0.9926    | 0.9902 |
| 0.0226        | 14.0  | 13482 | 0.9973   | 0.9913 | 0.0286          | 0.9930    | 0.9895 |
| 0.0195        | 15.0  | 14445 | 0.9972   | 0.9897 | 0.0271          | 0.9909    | 0.9885 |
| 0.0168        | 16.0  | 15408 | 0.9977   | 0.9925 | 0.0243          | 0.9937    | 0.9913 |
| 0.0145        | 17.0  | 16371 | 0.9977   | 0.9916 | 0.0249          | 0.9923    | 0.9909 |
| 0.0128        | 18.0  | 17334 | 0.9977   | 0.9925 | 0.0226          | 0.9934    | 0.9916 |
| 0.0116        | 19.0  | 18297 | 0.9975   | 0.9904 | 0.0234          | 0.9926    | 0.9881 |
| 0.0096        | 20.0  | 19260 | 0.9977   | 0.9918 | 0.0226          | 0.9930    | 0.9906 |
| 0.0082        | 21.0  | 20223 | 0.9977   | 0.9920 | 0.0248          | 0.9930    | 0.9909 |
| 0.0075        | 22.0  | 21186 | 0.9978   | 0.9925 | 0.0214          | 0.9934    | 0.9916 |
| 0.0066        | 23.0  | 22149 | 0.9975   | 0.9907 | 0.0238          | 0.9930    | 0.9885 |
| 0.006         | 24.0  | 23112 | 0.9975   | 0.9904 | 0.0243          | 0.9919    | 0.9888 |
| 0.005         | 25.0  | 24075 | 0.9977   | 0.9930 | 0.0226          | 0.9937    | 0.9923 |
| 0.0044        | 26.0  | 25038 | 0.9978   | 0.9923 | 0.0212          | 0.9933    | 0.9913 |
| 0.0037        | 27.0  | 26001 | 0.9978   | 0.9918 | 0.0242          | 0.9933    | 0.9902 |
| 0.0032        | 28.0  | 26964 | 0.9978   | 0.9921 | 0.0215          | 0.9923    | 0.9920 |
| 0.0029        | 29.0  | 27927 | 0.9977   | 0.9928 | 0.0226          | 0.9944    | 0.9913 |
| 0.0024        | 30.0  | 28890 | 0.9977   | 0.9925 | 0.0218          | 0.9930    | 0.9920 |
| 0.0022        | 31.0  | 29853 | 0.9979   | 0.9930 | 0.0218          | 0.9930    | 0.9930 |
| 0.002         | 32.0  | 30816 | 0.9978   | 0.9934 | 0.0224          | 0.9930    | 0.9937 |
| 0.0017        | 33.0  | 31779 | 0.9982   | 0.9949 | 0.0216          | 0.9951    | 0.9948 |
| 0.0015        | 34.0  | 32742 | 0.9982   | 0.9941 | 0.0225          | 0.9944    | 0.9937 |
| 0.0015        | 35.0  | 33705 | 0.9978   | 0.9939 | 0.0227          | 0.9944    | 0.9934 |
| 0.0014        | 36.0  | 34668 | 0.9980   | 0.9937 | 0.0224          | 0.9941    | 0.9934 |
| 0.001         | 37.0  | 35631 | 0.9980   | 0.9937 | 0.0228          | 0.9947    | 0.9927 |
| 0.0009        | 38.0  | 36594 | 0.9980   | 0.9937 | 0.0228          | 0.9941    | 0.9934 |
| 0.0008        | 39.0  | 37557 | 0.9979   | 0.9934 | 0.0224          | 0.9944    | 0.9923 |
| 0.0007        | 40.0  | 38520 | 0.9979   | 0.9925 | 0.0245          | 0.9944    | 0.9906 |
| 0.0007        | 41.0  | 39483 | 0.9980   | 0.9934 | 0.0222          | 0.9941    | 0.9927 |
| 0.0005        | 42.0  | 40446 | 0.9977   | 0.9927 | 0.0237          | 0.9934    | 0.9920 |
| 0.0005        | 43.0  | 41409 | 0.9977   | 0.9921 | 0.0229          | 0.9937    | 0.9906 |
| 0.0006        | 44.0  | 42372 | 0.9978   | 0.9934 | 0.0237          | 0.9944    | 0.9923 |
| 0.0005        | 45.0  | 43335 | 0.9979   | 0.9935 | 0.0227          | 0.9944    | 0.9927 |
| 0.0004        | 46.0  | 44298 | 0.9979   | 0.9934 | 0.0228          | 0.9937    | 0.9930 |
| 0.0004        | 47.0  | 45261 | 0.9979   | 0.9941 | 0.0241          | 0.9948    | 0.9934 |
| 0.0003        | 48.0  | 46224 | 0.9977   | 0.9921 | 0.0242          | 0.9937    | 0.9906 |
| 0.0004        | 49.0  | 47187 | 0.9979   | 0.9937 | 0.0235          | 0.9944    | 0.9930 |
| 0.0003        | 50.0  | 48150 | 0.9978   | 0.9930 | 0.0234          | 0.9934    | 0.9927 |
| 0.0003        | 51.0  | 49113 | 0.9979   | 0.9934 | 0.0239          | 0.9944    | 0.9923 |
| 0.0003        | 52.0  | 50076 | 0.9978   | 0.9927 | 0.0240          | 0.9940    | 0.9913 |
| 0.0002        | 53.0  | 51039 | 0.9976   | 0.9923 | 0.0242          | 0.9930    | 0.9916 |
| 0.0003        | 54.0  | 52002 | 0.9978   | 0.9927 | 0.0239          | 0.9934    | 0.9920 |
| 0.0002        | 55.0  | 52965 | 0.9978   | 0.9927 | 0.0240          | 0.9934    | 0.9920 |
| 0.0002        | 56.0  | 53928 | 0.9979   | 0.9934 | 0.0238          | 0.9944    | 0.9923 |
| 0.0002        | 57.0  | 54891 | 0.9978   | 0.9927 | 0.0240          | 0.9940    | 0.9913 |
| 0.0002        | 58.0  | 55854 | 0.9978   | 0.9927 | 0.0240          | 0.9940    | 0.9913 |
| 0.0002        | 59.0  | 56817 | 0.9978   | 0.9927 | 0.0240          | 0.9940    | 0.9913 |
| 0.0002        | 60.0  | 57780 | 0.9978   | 0.9927 | 0.0240          | 0.9940    | 0.9913 |


### Framework versions

- Transformers 4.45.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.1
- Tokenizers 0.20.1
