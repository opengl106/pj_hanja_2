"한문 교육용 기초 한자", "漢文 敎育用 基礎 漢字"
"한국의 수도는 [MASK]이다"

首先，我的问题正确的做法不是 seq2seq, 而是 token classification, 正如 NER 等经典问题。NER 等问题也是从韩语的语言上下文中寻找专有名词，也是需要对上下文的理解，也是 NLP 任务，也是使用 BERT 能做的更好的东西。一定程度上，这和“标记汉字词”很像：NE 词汇与日常词汇间存在泾渭分明的区别，正如汉字词汇与大韩词汇。

"""
import torch
torch.cuda.is_available()

list(labels_to_hanja_words.values())[0:250]
list(labels_to_hanja_words.values())[250:500]
labeled_inputs['train'][4181]
labeled_inputs['test'][254]

"""
from predict import predict
predict('디두모라 하는 도마가 다른 제자들에게 말하되 우리도 주와 함께 죽으러 가자 하니라')
predict(['디두모라 하는 도마가 다른 제자들에게 말하되 우리도 주와 함께 죽으러 가자 하니라', '돌을 옮겨 놓으니 예수께서 눈을 들어 우러러 보시고 가라사대 아버 지여 내 말을 들으신 것을 감사하나이다'])


import time
import codecs
strings = []
i = 0
for line in codecs.open('data/bible_ko.tsv', 'r', 'utf-8'):
    if len(line) <= 500:
        strings.append(line.strip().split("\t")[0])
    i += 1
    if i >= 1000:
        break

strings[962]

t = time.time()
c = predict(strings)
dt = time.time() - t
dt
c[962]
